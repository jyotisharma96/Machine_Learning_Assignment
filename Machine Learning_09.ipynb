{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c5b3f9",
   "metadata": {},
   "source": [
    "1.What is feature engineering, and how does it work? Explain the various aspects of feature\n",
    "engineering in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fe3082",
   "metadata": {},
   "source": [
    "Answer- Feature engineering is the process of transforming raw data into a suitable format that can be effectively utilized by machine learning algorithms. It involves creating, selecting, or transforming features (input variables) to improve the performance, interpretability, and efficiency of a machine learning model. Feature engineering plays a crucial role in the success of a machine learning project as it directly impacts the quality and relevance of the input data.\n",
    "\n",
    "Various aspects of feature engineering include:\n",
    "\n",
    "1. Understanding the problem statement.\n",
    "\n",
    "2. Train a baseline model and calculate its performance metric\n",
    "\n",
    "3. Reaching a conclusion about what features to create\n",
    "\n",
    "4. Create features\n",
    "\n",
    "5. Calculating how good new features contribute by training and testing and comparing the baseline performance metric\n",
    "\n",
    "6. Improve the features if need be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5656e81d",
   "metadata": {},
   "source": [
    "2.What is feature selection, and how does it work? What is the aim of it? What are the various\n",
    "methods of function selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea0461d",
   "metadata": {},
   "source": [
    "Answer- Feature selection is the process of selecting a subset of relevant features from the original set of features in a dataset. It aims to reduce dimensionality, improve model performance, enhance interpretability, and alleviate computational complexity. Feature selection helps to focus on the most informative and discriminative features while eliminating irrelevant or redundant ones. \n",
    "\n",
    "Feature selection operates on the premise that not all features contribute equally to the predictive power of a model. By selecting a subset of relevant features, the dimensionality of the feature space is reduced, which can lead to improved model efficiency and generalization performance.\n",
    "\n",
    "Feature selection methods typically evaluate the importance or usefulness of individual features or feature subsets based on certain criteria, such as statistical measures, model-based evaluations, or domain knowledge. The selected features are then used as input for the subsequent modeling process.\n",
    "\n",
    "The primary goal of feature selection is to identify the most informative and relevant subset of features that have the strongest predictive power and minimize the inclusion of irrelevant or redundant features.\n",
    "Feature selection aims to improve the model's performance by reducing overfitting, reducing noise and data complexity, increasing interpretability, reducing computational and storage requirements, and enhancing the generalization ability of the model.\n",
    "\n",
    "Various methods of feature selection:\n",
    "\n",
    "1. Filter Methods: Filter methods evaluate the relevance of features independently of the learning algorithm. They employ statistical measures or domain knowledge to assess the importance of features. Examples include correlation-based feature selection, chi-square test, mutual information, and information gain.\n",
    "\n",
    "\n",
    "2. Wrapper Methods: Wrapper methods use a specific learning algorithm to evaluate the performance of different subsets of features. They involve iterative feature selection by training and evaluating models using different feature subsets. Examples include recursive feature elimination (RFE) and forward/backward stepwise selection.\n",
    "\n",
    "\n",
    "3. Embedded Methods: Embedded methods incorporate feature selection within the learning algorithm itself. These methods aim to select the most informative features during the training process, directly optimizing the model's performance. Examples include L1 regularization (Lasso), decision tree-based feature importance, and regularization-based methods like Elastic Net.\n",
    "\n",
    "\n",
    "4. Hybrid Methods: Hybrid methods combine multiple feature selection techniques to leverage the strengths of different approaches. They often use a combination of filter, wrapper, or embedded methods to identify the most relevant features. Ensemble methods, such as Random Forest or Gradient Boosting, can also be used for feature selection by evaluating feature importance across multiple models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d2b27",
   "metadata": {},
   "source": [
    "3.Describe the function selection filter and wrapper approaches. State the pros and cons of each\n",
    "approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb8859",
   "metadata": {},
   "source": [
    "Answer- Filter methods rely on finding the statistics for each feature to select the features with highest contributions to the machine learning model. These statistics are calculated for both categorical and numerical data. For numerical data, statistics like Correlation coefficients and for categorical data, statistics like the Chi-Square test are applied between two features to find probability of correlation(linear and non-linear).\n",
    "\n",
    "Advantage:--- Less computational cost Disadvantage:--- Less accurate, cannot handle multicollinearity\n",
    "\n",
    "Wrapper methods rely on an iterative approach where model is trained over subsets of features and tested using a performance metric, using which we can decide to keep features or discard them by comparing the recent performance score with the last score.\n",
    "\n",
    "Advantage:--- Most accurate Disadvantage:--- Computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ef3fbf",
   "metadata": {},
   "source": [
    "4.Describe the overall feature selection process. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0c319",
   "metadata": {},
   "source": [
    "Answer-  Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.\n",
    "\n",
    "Feature extraction aims to transform the original set of features into a new set of features by applying mathematical or statistical techniques. One widely used algorithm for feature extraction is Principal Component Analysis (PCA). The key principle of PCA is to find a new set of uncorrelated variables, called principal components, that capture the maximum variance in the data.\n",
    "\n",
    "For example, let's consider a dataset with several correlated features representing different physical measurements of a chemical process. By applying PCA, we can identify the principal components that explain most of the variance in the data. These principal components are linear combinations of the original features and are arranged in descending order of variance. We can choose to retain a certain number of principal components that capture a significant portion of the variance (e.g., 95% of the variance).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9b871b",
   "metadata": {},
   "source": [
    "5.Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e5ddd",
   "metadata": {},
   "source": [
    "Answer- Text classification is the problem of assigning categories to text data according to its content. The most important part of text classification is feature engineering: the process of creating features for a machine learning model from raw text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab8bb19",
   "metadata": {},
   "source": [
    "6.What makes cosine similarity a good metric for text categorization? A document-term matrix has\n",
    "two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in\n",
    "cosine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0933c713",
   "metadata": {},
   "source": [
    "Answer- Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together.\n",
    "\n",
    "Cosine similarity is the cosine of the angle between two n-dimensional vectors in an n-dimensional space. It is the dot product of the two vectors divided by the product of the two vectors' lengths (or magnitudes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4631f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5467c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = (2*2)+(3*1)+(2*0)+(0*0)+(2*3)+(3*2)+(3*1)+(0*3)+(1*1)\n",
    "denominator = sqrt(4+9+4+4+9+9+1)*sqrt(4+1+9+4+1+9+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78030698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6753032524419089\n"
     ]
    }
   ],
   "source": [
    "similarity = numerator/denominator\n",
    "print(similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be107d7b",
   "metadata": {},
   "source": [
    "7.What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
    "calculate the Hamming gap.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
    "0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c87db22",
   "metadata": {},
   "source": [
    "Answer- Hamming distance is calculated as the number of bit places where any two bit strings are different.\n",
    "\n",
    "Hamming distance between 10001011 and 11001111 would be 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f27d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n",
      "0.625\n"
     ]
    }
   ],
   "source": [
    "A = (1, 1, 0, 0, 1, 0, 1, 1)\n",
    "C = (1, 0, 0, 1, 1, 0, 0, 1)\n",
    "\n",
    "Jac_ind1 = (2/4)*100\n",
    "SMC1 = 5/8\n",
    "print(Jac_ind1)\n",
    "print(SMC1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c0a3860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n",
      "0.375\n"
     ]
    }
   ],
   "source": [
    "B = (1, 1, 0, 0, 0, 1, 1, 1)\n",
    "C = (1, 0, 0, 1, 1, 0, 0, 1)\n",
    "\n",
    "Jac_ind2 = (2/4)*100\n",
    "SMC2 = 3/8\n",
    "print(Jac_ind2)\n",
    "print(SMC2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd45b860",
   "metadata": {},
   "source": [
    "8.State what is meant by &quot;high-dimensional data set&quot;? Could you offer a few real-life examples?\n",
    "What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
    "What can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e4790",
   "metadata": {},
   "source": [
    "Answer- High dimensional data set means that there are high number of features present in the dataset, so high that dimensionality of the dataset becomes detrimental to the learning phase of the machine learning model. In a high dimensional data, number of features can be greater than or equal to the number of observations.\n",
    "\n",
    "Real life examples:\n",
    "\n",
    "Gene sequencing data, spatio-temporal data, sociology based data.\n",
    "\n",
    "The time space complexity is different for each machine learning algorithm, but each suffers from high dimensional data. The curse of dimensionality is at play here. High dimensional data not only slows down the training phase because of the number of computations the algorithm needs to perform increases in quadratic and cubic orders for some algorithms, it also affects the accuracy of algorithms due to presence of correlations that do not make sense and also results in overfitting.\n",
    "\n",
    "High dimensional data can be processed using feature selection procedures, or dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b51bc9",
   "metadata": {},
   "source": [
    "9.Make a few quick notes on:\n",
    "\n",
    "\n",
    "i. PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "\n",
    "ii. Use of vectors\n",
    "\n",
    "\n",
    "iii. Embedded technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8ec51d",
   "metadata": {},
   "source": [
    "Answer- \n",
    "\n",
    "i. __PCA is an acronym for Personal Computer Analysis__- PCA stands for Principal Component Analysis, not Personal Computer Analysis. PCA is a widely used dimensionality reduction technique in machine learning and data analysis. It helps to transform a high-dimensional dataset into a lower-dimensional representation while preserving the most important information. By finding the principal components, which are linear combinations of the original features, PCA enables visualization, noise reduction, and improved computational efficiency.\n",
    "\n",
    "\n",
    "ii. __Use of vectors__- Vectors play a crucial role in various areas of mathematics, physics, and computer science, including machine learning. In the context of machine learning, vectors are used to represent data points or feature sets. Vectors can be represented as arrays or lists of numbers and can have different dimensions. They allow us to perform mathematical operations like addition, subtraction, dot product, and normalization. Vectors are commonly used to represent features, weights, and predictions in machine learning algorithms.\n",
    "\n",
    "\n",
    "iii. __Embedded technique__- Embedded techniques in feature selection refer to methods that perform feature selection and model training simultaneously. Unlike filter and wrapper methods that treat feature selection as a separate step, embedded techniques incorporate feature selection directly into the model learning process. This integration enables the model to determine the importance of features during training based on their contribution to the objective function. Embedded techniques include algorithms like LASSO (Least Absolute Shrinkage and Selection Operator) and Ridge Regression, which add regularization terms to the objective function to encourage feature selection. These methods offer the advantage of selecting relevant features while optimizing the model, reducing the risk of overfitting, and improving interpretability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3944a9",
   "metadata": {},
   "source": [
    "10.Make a comparison between:\n",
    "\n",
    "    \n",
    "i. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "\n",
    "ii. Function selection methods: filter vs. wrapper\n",
    "\n",
    "    \n",
    "iii. SMC vs. Jaccard coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc2954",
   "metadata": {},
   "source": [
    "Answer- \n",
    "\n",
    "i. __Sequential backward exclusion vs. sequential forward selection__:\n",
    "\n",
    "Sequential backward exclusion is a feature selection method that starts with all features and iteratively removes one feature at a time based on a predefined criterion, typically using a machine learning model's performance. It continues removing features until a stopping criterion is met or a desired number of features remain. This approach starts with a full feature set and gradually reduces it.\n",
    "\n",
    "Sequential forward selection, on the other hand, begins with an empty feature set and incrementally adds one feature at a time based on a performance criterion. It continues adding features until a stopping criterion is met or a desired number of features are selected. This approach starts with a small feature set and progressively expands it.\n",
    "\n",
    "ii. __Function selection methods: filter vs. wrapper__:\n",
    "\n",
    "Filter methods in feature selection evaluate the relevance of features independently of any specific machine learning algorithm. These methods rely on statistical measures or heuristics to rank or score features based on their individual characteristics. Examples include correlation, chi-square, and information gain. Filter methods are computationally efficient and can handle a large number of features, but they may not consider feature dependencies or the specific learning algorithm.\n",
    "\n",
    "Wrapper methods, on the other hand, incorporate the learning algorithm as part of the feature selection process. They use the performance of the learning algorithm on different subsets of features to evaluate their quality. Wrapper methods typically involve an iterative search procedure to find an optimal subset of features that maximizes the model's performance. Wrapper methods can consider feature dependencies and the specific learning algorithm but are computationally more expensive than filter methods.\n",
    "\n",
    "iii. __SMC vs. Jaccard coefficient__:\n",
    "\n",
    "SMC (Similarity Matching Coefficient) is a similarity measure used in feature selection to quantify the similarity between two sets of features. It is calculated as the ratio of the number of common features between the sets to the maximum number of features in either set. SMC ranges from 0 to 1, where 0 indicates no common features and 1 indicates identical sets of features.\n",
    "\n",
    "The Jaccard coefficient, also a similarity measure, is used to measure the similarity between two sets. It is calculated as the ratio of the number of common elements between the sets to the total number of distinct elements in the sets. The Jaccard coefficient also ranges from 0 to 1, with 0 indicating no common elements and 1 indicating identical sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2184105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d578d58c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
